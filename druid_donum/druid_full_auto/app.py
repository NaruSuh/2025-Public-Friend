#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬ - Streamlit Web App
"""

import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import time
from main import ForestBidCrawler
import os
from io import BytesIO
from functools import lru_cache

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬",
    page_icon="ğŸŒ²",
    layout="wide"
)

# ì œëª©
st.title("ğŸŒ² ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬")
st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ í¬ë¡¤ë§ ì„¤ì •")

# ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì • (ë…„ ë‹¨ìœ„)
years = st.sidebar.slider(
    "ìˆ˜ì§‘ ê¸°ê°„ (ë…„)",
    min_value=1,
    max_value=10,
    value=1,
    step=1,
    help="ì§€ë‚œ ëª‡ ë…„ ë™ì•ˆì˜ ì…ì°°ì •ë³´ë¥¼ ìˆ˜ì§‘í• ì§€ ì„¤ì •í•©ë‹ˆë‹¤."
)

# ë…„ì„ ì¼ë¡œ ë³€í™˜
days = years * 365

# ìš”ì²­ ê°„ê²© ì„¤ì •
delay = st.sidebar.slider(
    "ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)",
    min_value=0.5,
    max_value=3.0,
    value=1.0,
    step=0.5,
    help="ê° ìš”ì²­ ì‚¬ì´ì˜ ëŒ€ê¸° ì‹œê°„ì…ë‹ˆë‹¤. ë„ˆë¬´ ì§§ìœ¼ë©´ ì„œë²„ì—ì„œ ì°¨ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
)

page_delay = st.sidebar.slider(
    "í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì´ˆ)",
    min_value=0.5,
    max_value=5.0,
    value=2.0,
    step=0.5,
    help="í˜ì´ì§€ ì´ë™ ì‹œ ëŒ€ê¸° ì‹œê°„ì…ë‹ˆë‹¤."
)

# ì‚¬ì´ë“œë°” í•˜ë‹¨: í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬
if st.session_state.crawl_history:
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“‚ ì´ì „ í¬ë¡¤ë§ ê²°ê³¼")

    for idx, item in enumerate(reversed(st.session_state.crawl_history)):
        # timestampë¥¼ unique keyë¡œ ì‚¬ìš©
        unique_key = item['timestamp'].replace(':', '').replace(' ', '').replace('-', '')

        with st.sidebar.expander(f"ğŸ• {item['timestamp'].replace('_', ' ')}", expanded=False):
            st.write(f"**ìˆ˜ì§‘ ê¸°ê°„**: {item['period']}")
            st.write(f"**í•­ëª© ìˆ˜**: {item['total_items']}ê°œ")

            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            col_a, col_b = st.columns(2)

            with col_a:
                # Excel ë‹¤ìš´ë¡œë“œ (ìºì‹± ì‚¬ìš©)
                try:
                    # DataFrameì„ dictë¡œ ë³€í™˜í•˜ì—¬ ìºì‹± ê°€ëŠ¥í•˜ê²Œ
                    df_dict = item['data'].to_dict()
                    excel_data = generate_excel_data(df_dict, item['timestamp'])

                    st.download_button(
                        label="ğŸ“¥ Excel",
                        data=excel_data,
                        file_name=f"ì‚°ë¦¼ì²­_ì…ì°°_{item['timestamp']}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key=f"excel_{unique_key}"
                    )
                except Exception as e:
                    st.error(f"Excel ìƒì„± ì‹¤íŒ¨")

            with col_b:
                # CSV ë‹¤ìš´ë¡œë“œ (ìºì‹± ì‚¬ìš©)
                try:
                    # DataFrameì„ dictë¡œ ë³€í™˜í•˜ì—¬ ìºì‹± ê°€ëŠ¥í•˜ê²Œ
                    df_dict = item['data'].to_dict()
                    csv = generate_csv_data(df_dict, item['timestamp'])

                    st.download_button(
                        label="ğŸ“¥ CSV",
                        data=csv,
                        file_name=f"ì‚°ë¦¼ì²­_ì…ì°°_{item['timestamp']}.csv",
                        mime="text/csv",
                        key=f"csv_{unique_key}"
                    )
                except Exception as e:
                    st.error(f"CSV ìƒì„± ì‹¤íŒ¨")

# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([2, 1])

with col1:
    st.info(f"""
    ğŸ“… **ìˆ˜ì§‘ ê¸°ê°„**: ìµœê·¼ {years}ë…„ (ì•½ {days}ì¼)
    â±ï¸ **ìš”ì²­ ë”œë ˆì´**: {delay}ì´ˆ
    ğŸ“„ **í˜ì´ì§€ ë”œë ˆì´**: {page_delay}ì´ˆ
    ğŸ¯ **ëŒ€ìƒ**: ì‚°ë¦¼ì²­ ì…ì°°ê³µê³  ê²Œì‹œíŒ
    """)

with col2:
    st.metric("ìˆ˜ì§‘ ê¸°ì¤€ì¼", (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d'))

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'crawl_logs' not in st.session_state:
    st.session_state.crawl_logs = []
if 'crawl_data' not in st.session_state:
    st.session_state.crawl_data = None
if 'crawl_completed' not in st.session_state:
    st.session_state.crawl_completed = False
if 'crawl_history' not in st.session_state:
    st.session_state.crawl_history = []  # ì™„ë£Œëœ í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬

# ë¡œê·¸ ì¶”ê°€ í•¨ìˆ˜
def add_log(message, log_type="INFO"):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    st.session_state.crawl_logs.append(f"[{timestamp}] [{log_type}] {message}")

# Excel ë°ì´í„° ìƒì„± í•¨ìˆ˜ (ìºì‹±)
@st.cache_data
def generate_excel_data(df_dict, timestamp):
    """DataFrameì„ Excel ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜ (ìºì‹±ë¨)"""
    df = pd.DataFrame(df_dict)
    buffer = BytesIO()
    df.to_excel(buffer, index=False, engine='openpyxl')
    return buffer.getvalue()

# CSV ë°ì´í„° ìƒì„± í•¨ìˆ˜ (ìºì‹±)
@st.cache_data
def generate_csv_data(df_dict, timestamp):
    """DataFrameì„ CSVë¡œ ë³€í™˜ (ìºì‹±ë¨)"""
    df = pd.DataFrame(df_dict)
    return df.to_csv(index=False, encoding='utf-8-sig')

# í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜ (ì¤‘ë³µ ì œê±°)
def run_crawling(years, days, delay, page_delay):
    """í¬ë¡¤ë§ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
    # yearsë¥¼ ì „ì—­ì²˜ëŸ¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ ì €ì¥
    crawl_years = years
    # ì§„í–‰ ìƒí™© í‘œì‹œ ì˜ì—­
    progress_bar = st.progress(0)
    status_text = st.empty()
    info_text = st.empty()
    result_placeholder = st.empty()

    try:
        add_log(f"í¬ë¡¤ë§ ì‹œì‘ - ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {years}ë…„ ({days}ì¼)")
        add_log(f"ì„¤ì • - ìš”ì²­ ë”œë ˆì´: {delay}ì´ˆ, í˜ì´ì§€ ë”œë ˆì´: {page_delay}ì´ˆ")

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = ForestBidCrawler(
            days=days,
            delay=delay,
            page_delay=page_delay
        )

        status_text.info("ğŸ”„ í¬ë¡¤ë§ ì‹œì‘...")

        # í¬ë¡¤ë§ ì‹¤í–‰
        page_index = 1
        should_continue = True
        total_pages_estimate = 50

        while should_continue:
            info_text.text(f"ğŸ“„ í˜ì´ì§€ {page_index} ì²˜ë¦¬ ì¤‘...")
            add_log(f"í˜ì´ì§€ {page_index} ì²˜ë¦¬ ì‹œì‘")

            # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            params = {
                'mn': 'NKFS_04_01_04',
                'bbsId': 'BBSMSTR_1033',
                'pageIndex': page_index,
                'pageUnit': 10
            }

            soup = crawler.fetch_page(crawler.LIST_URL, params)

            if not soup:
                add_log(f"í˜ì´ì§€ {page_index} ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨", "ERROR")
                break

            items = crawler.parse_list_page(soup)

            if not items:
                add_log(f"í˜ì´ì§€ {page_index}ì— í•­ëª© ì—†ìŒ", "WARNING")
                break

            add_log(f"í˜ì´ì§€ {page_index}ì—ì„œ {len(items)}ê°œ í•­ëª© ë°œê²¬")

            # ê° í•­ëª© ì²˜ë¦¬
            for idx, item in enumerate(items, 1):
                # ë‚ ì§œ ì²´í¬
                if item['post_date'] and item['post_date'] < crawler.cutoff_date:
                    add_log(f"ê¸°ì¤€ì¼ ì´ì „ ê²Œì‹œê¸€ ë„ë‹¬ ({item['post_date_str']}) - í¬ë¡¤ë§ ì¢…ë£Œ", "INFO")
                    should_continue = False
                    break

                # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                if item['detail_url']:
                    time.sleep(crawler.delay)
                    detail_soup = crawler.fetch_page(item['detail_url'])

                    if detail_soup:
                        detail_data = crawler.parse_detail_page(detail_soup, item)
                        crawler.data.append(detail_data)
                        crawler.total_items += 1
                        add_log(f"í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ: {item['title'][:30]}...")
                    else:
                        add_log(f"ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {item['title'][:30]}...", "ERROR")
                        crawler.data.append(item)
                        crawler.total_items += 1
                else:
                    crawler.data.append(item)
                    crawler.total_items += 1

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = min(page_index / total_pages_estimate, 0.99)
                progress_bar.progress(progress)

            # ì¤‘ê°„ ê²°ê³¼ í‘œì‹œ
            if crawler.data:
                df = pd.DataFrame(crawler.data)
                result_placeholder.dataframe(
                    df.head(20),
                    use_container_width=True,
                    hide_index=True
                )

            if should_continue:
                page_index += 1
                time.sleep(crawler.page_delay)

            # ìµœëŒ€ í˜ì´ì§€ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            if page_index > 100:
                add_log("ìµœëŒ€ í˜ì´ì§€ ìˆ˜(100) ë„ë‹¬ - í¬ë¡¤ë§ ì¢…ë£Œ", "WARNING")
                break

        progress_bar.progress(1.0)
        status_text.success(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {crawler.total_items}ê°œ í•­ëª© ìˆ˜ì§‘")
        add_log(f"í¬ë¡¤ë§ ì™„ë£Œ - ì´ {crawler.total_items}ê°œ í•­ëª© ìˆ˜ì§‘")

        # ë°ì´í„° ì²˜ë¦¬ ë° ë°˜í™˜
        if crawler.data:
            df = pd.DataFrame(crawler.data)

            # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
            columns = [
                'number', 'title', 'forest_office', 'department',
                'manager', 'contact', 'post_date_str', 'views',
                'has_attachment', 'detail_url'
            ]
            columns = [col for col in columns if col in df.columns]
            df = df[columns]

            # ì»¬ëŸ¼ëª… í•œê¸€í™”
            df.columns = [
                'ë²ˆí˜¸', 'ì œëª©', 'ë‹´ë‹¹ì‚°ë¦¼ì²­', 'ë‹´ë‹¹ë¶€ì„œ',
                'ë‹´ë‹¹ì', 'ì—°ë½ì²˜', 'ê³µê³ ì¼ì', 'ì¡°íšŒìˆ˜',
                'ì²¨ë¶€', 'URL'
            ][:len(columns)]

            # ì„¸ì…˜ì— ë°ì´í„° ì €ì¥
            st.session_state.crawl_data = df
            st.session_state.crawl_completed = True

            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ìœ ì§€)
            history_item = {
                'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),  # íŒŒì¼ëª… ì•ˆì „
                'data': df.copy(),
                'total_items': len(df),
                'period': f"{crawl_years}ë…„"
            }
            st.session_state.crawl_history.append(history_item)

            # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if len(st.session_state.crawl_history) > 5:
                st.session_state.crawl_history.pop(0)  # ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°

            # ê²°ê³¼ í‘œì‹œ
            st.markdown("---")
            st.subheader("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")

            # í†µê³„ ì •ë³´
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ í•­ëª© ìˆ˜", len(df))
            with col2:
                st.metric("ë‹´ë‹¹ì‚°ë¦¼ì²­ ìˆ˜", df['ë‹´ë‹¹ì‚°ë¦¼ì²­'].nunique() if 'ë‹´ë‹¹ì‚°ë¦¼ì²­' in df.columns else 0)
            with col3:
                st.metric("ìˆ˜ì§‘ í˜ì´ì§€", page_index)
            with col4:
                # í‰ê·  ì¡°íšŒìˆ˜ ê³„ì‚° (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                avg_views = 0
                if 'ì¡°íšŒìˆ˜' in df.columns and len(df) > 0:
                    try:
                        views_numbers = df['ì¡°íšŒìˆ˜'].astype(str).str.extract('(\d+)')[0].astype(float)
                        avg_views = int(views_numbers.mean()) if not views_numbers.isna().all() else 0
                    except:
                        avg_views = 0
                st.metric("í‰ê·  ì¡°íšŒìˆ˜", avg_views)

            # ë°ì´í„° í…Œì´ë¸”
            st.dataframe(df, use_container_width=True, hide_index=True)

            return True
        else:
            return False

    except Exception as e:
        status_text.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        add_log(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
        st.exception(e)
        return False

# í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼
col_btn1, col_btn2 = st.columns(2)

with col_btn1:
    start_crawl = st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", type="primary", use_container_width=True)

with col_btn2:
    export_data = st.button("ğŸ“¥ í¬ë¡¤ë§ ë° ì™„ë£Œì‹œ ì—‘ì…€íŒŒì¼ ì‘ì„±", type="secondary", use_container_width=True)

# í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼
if start_crawl:
    # ì´ˆê¸°í™”
    st.session_state.crawl_logs = []
    st.session_state.crawl_data = None
    st.session_state.crawl_completed = False

    # í¬ë¡¤ë§ ì‹¤í–‰
    run_crawling(years, days, delay, page_delay)

# "í¬ë¡¤ë§ ë° ì™„ë£Œì‹œ ì—‘ì…€íŒŒì¼ ì‘ì„±" ë²„íŠ¼ ê¸°ëŠ¥
if export_data:
    # ì´ˆê¸°í™”
    st.session_state.crawl_logs = []
    st.session_state.crawl_data = None
    st.session_state.crawl_completed = False

    # í¬ë¡¤ë§ ì‹¤í–‰
    run_crawling(years, days, delay, page_delay)

# í¬ë¡¤ë§ ì™„ë£Œ í›„ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ (ë‘ ë²„íŠ¼ ëª¨ë‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
if st.session_state.crawl_completed and st.session_state.crawl_data is not None:
    st.markdown("---")
    st.subheader("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")

    df = st.session_state.crawl_data
    col1, col2 = st.columns(2)

    with col1:
        # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
        try:
            buffer = BytesIO()
            df.to_excel(buffer, index=False, engine='openpyxl')
            excel_data = buffer.getvalue()

            st.download_button(
                label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.xlsx)",
                data=excel_data,
                file_name=f"ì‚°ë¦¼ì²­_ì…ì°°ì •ë³´_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
        except Exception as e:
            st.error(f"ì—‘ì…€ ìƒì„± ì‹¤íŒ¨: {e}")

    with col2:
        # CSV ë‹¤ìš´ë¡œë“œ
        csv = df.to_csv(index=False, encoding='utf-8-sig')

        st.download_button(
            label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.csv)",
            data=csv,
            file_name=f"ì‚°ë¦¼ì²­_ì…ì°°ì •ë³´_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True
        )

# ë¡œê·¸ ë·°ì–´ (ì¢Œì¸¡ í•˜ë‹¨)
if st.session_state.crawl_logs and len(st.session_state.crawl_logs) > 0:
    st.markdown("---")

    log_col1, log_col2 = st.columns([3, 1])

    with log_col1:
        st.subheader("ğŸ“‹ í¬ë¡¤ë§ ë¡œê·¸")

    with log_col2:
        # ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        log_content = "# ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ë§ ë¡œê·¸\n\n"
        log_content += f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        log_content += "## ë¡œê·¸ ë‚´ì—­\n\n"
        log_content += "\n".join(st.session_state.crawl_logs)

        st.download_button(
            label="ğŸ“¥ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ (.md)",
            data=log_content,
            file_name=f"í¬ë¡¤ë§_ë¡œê·¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown",
            use_container_width=True
        )

    # ë¡œê·¸ í‘œì‹œ (í™•ì¥ ê°€ëŠ¥í•œ í˜•íƒœ)
    with st.expander("ë¡œê·¸ ë³´ê¸°", expanded=False):
        log_text = "\n".join(st.session_state.crawl_logs)
        st.text_area(
            "ë¡œê·¸ ë‚´ìš©",
            value=log_text,
            height=300,
            disabled=True,
            label_visibility="collapsed"
        )

# ì‚¬ìš© ì•ˆë‚´
st.markdown("---")
st.markdown("""
### ğŸ“– ì‚¬ìš© ì•ˆë‚´

1. **ì™¼ìª½ ì‚¬ì´ë“œë°”**ì—ì„œ í¬ë¡¤ë§ ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”.
2. **í¬ë¡¤ë§ ì‹œì‘** ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.
3. ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. ì™„ë£Œ í›„ **ì—‘ì…€/CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ** ë²„íŠ¼ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì„¸ìš”.
5. **ë¡œê·¸ ë³´ê¸°**ì—ì„œ í¬ë¡¤ë§ ê³¼ì •ì„ í™•ì¸í•˜ê³ , ë¡œê·¸ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### âš ï¸ ì£¼ì˜ì‚¬í•­

- í¬ë¡¤ë§ì— ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ìˆ˜ì§‘ ê¸°ê°„ì— ë”°ë¼ ìˆ˜ ë¶„ ~ ìˆ˜ì‹­ ë¶„)
- ì„œë²„ ë¶€í•˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì ì ˆí•œ ë”œë ˆì´ë¥¼ ì„¤ì •í•˜ì„¸ìš”
- ê³µê°œëœ ì •ë³´ë§Œ ìˆ˜ì§‘í•˜ë©°, ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”

### ğŸ“Š ìˆ˜ì§‘ ì •ë³´

- ë²ˆí˜¸, ì œëª©, ë‹´ë‹¹ì‚°ë¦¼ì²­, ë‹´ë‹¹ë¶€ì„œ
- ë‹´ë‹¹ì, ì—°ë½ì²˜, ê³µê³ ì¼ì, ì¡°íšŒìˆ˜
- ì²¨ë¶€íŒŒì¼ ì—¬ë¶€, ìƒì„¸ í˜ì´ì§€ URL
""")

# í‘¸í„°
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: gray;'>"
    "ğŸŒ² ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬ v1.0 | "
    "ì‚°ë¦¼ê³µí•™ ì „ë¬¸ê°€ / ì‚°ë¦¼í•™ ì—°êµ¬ì"
    "</div>",
    unsafe_allow_html=True
)
