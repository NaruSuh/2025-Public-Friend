#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬ - Streamlit Web App
"""

import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import time
from main import ForestBidCrawler
import os
from io import BytesIO
import logging
import traceback

APP_VERSION = "Ver 1.1.02"

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬",
    page_icon="ğŸŒ²",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ê°€ì¥ ë¨¼ì €!)
if 'crawl_logs' not in st.session_state:
    st.session_state.crawl_logs = []
if 'crawl_data' not in st.session_state:
    st.session_state.crawl_data = None
if 'crawl_completed' not in st.session_state:
    st.session_state.crawl_completed = False
if 'crawl_history' not in st.session_state:
    st.session_state.crawl_history = []  # ì™„ë£Œëœ í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬

# ë¡œê·¸ ì¶”ê°€ í•¨ìˆ˜
def add_log(message, log_type="INFO"):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    st.session_state.crawl_logs.append(f"[{timestamp}] [{log_type}] {message}")

# Excel ë°ì´í„° ìƒì„± í•¨ìˆ˜ (ìºì‹±)
def df_to_excel_bytes(df: pd.DataFrame) -> bytes:
    """DataFrameì„ Excel ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜(ì•ˆì •ì  ì§ë ¬í™”)"""
    buffer = BytesIO()
    df.to_excel(buffer, index=False, engine='openpyxl')
    return buffer.getvalue()

# CSV ë°ì´í„° ìƒì„± í•¨ìˆ˜ (ìºì‹±)
def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    return df.to_csv(index=False, encoding='utf-8-sig').encode('utf-8-sig')

# ì œëª©
st.markdown(
    f"<div style='text-align: left; font-weight: 600; color: #6c757d;'>{APP_VERSION}</div>",
    unsafe_allow_html=True
)
st.title("ğŸŒ² ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬")
st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ í¬ë¡¤ë§ ì„¤ì •")

# ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì • (ë‚ ì§œ ë²”ìœ„)
st.sidebar.markdown("#### ğŸ“… ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì •")

col_date1, col_date2 = st.sidebar.columns(2)

with col_date1:
    start_date = st.date_input(
        "ì‹œì‘ì¼",
        value=datetime.now() - timedelta(days=365),
        max_value=datetime.now(),
        help="í¬ë¡¤ë§ ì‹œì‘ ë‚ ì§œ"
    )

with col_date2:
    end_date = st.date_input(
        "ì¢…ë£Œì¼",
        value=datetime.now(),
        max_value=datetime.now(),
        help="í¬ë¡¤ë§ ì¢…ë£Œ ë‚ ì§œ"
    )

# ë‚ ì§œ ìœ íš¨ì„± ê²€ì‚¬
if start_date > end_date:
    st.sidebar.error("âš ï¸ ì‹œì‘ì¼ì´ ì¢…ë£Œì¼ë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤!")
    days = 0
else:
    days = (end_date - start_date).days
    st.sidebar.info(f"ğŸ“Š ìˆ˜ì§‘ ê¸°ê°„: **{days}ì¼** ({start_date} ~ {end_date})")

# ìš”ì²­ ê°„ê²© ì„¤ì •
delay = st.sidebar.slider(
    "ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)",
    min_value=0.5,
    max_value=3.0,
    value=1.0,
    step=0.5,
    help="ê° ìš”ì²­ ì‚¬ì´ì˜ ëŒ€ê¸° ì‹œê°„ì…ë‹ˆë‹¤. ë„ˆë¬´ ì§§ìœ¼ë©´ ì„œë²„ì—ì„œ ì°¨ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
)

page_delay = st.sidebar.slider(
    "í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì´ˆ)",
    min_value=0.5,
    max_value=5.0,
    value=2.0,
    step=0.5,
    help="í˜ì´ì§€ ì´ë™ ì‹œ ëŒ€ê¸° ì‹œê°„ì…ë‹ˆë‹¤."
)

# ì‚¬ì´ë“œë°”: ìºì‹œëœ íŒŒì¼ ë“œë¡­ë‹¤ìš´
st.sidebar.subheader("ğŸ“ ì§€ê¸ˆê¹Œì§€ ìºì‹œëœ íŒŒì¼")

history_items = list(reversed(st.session_state.crawl_history))
history_labels = ["ì„ íƒí•˜ì„¸ìš”"]
history_map = {}

for item in history_items:
    label = f"{item['timestamp'].replace('_', ' ')} Â· {item['period']} Â· {item['total_items']}ê°œ"
    history_labels.append(label)
    history_map[label] = item

if 'selected_history_label' not in st.session_state or st.session_state.selected_history_label not in history_labels:
    st.session_state.selected_history_label = history_labels[0]

selected_history_label = st.sidebar.selectbox(
    "ì§€ê¸ˆê¹Œì§€ ìºì‹œëœ íŒŒì¼",
    options=history_labels,
    key="selected_history_label"
)
if selected_history_label == history_labels[0]:
    if not history_items:
        st.sidebar.caption("ì•„ì§ ìºì‹œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
else:
    selected_history = history_map[selected_history_label]
    st.sidebar.markdown(
        f"**ìˆ˜ì§‘ ê¸°ê°„**: {selected_history['period']}  \
        **í•­ëª© ìˆ˜**: {selected_history['total_items']}ê°œ"
    )

    # selected_history['data']ëŠ” DataFrameìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆìŒ
    df = selected_history['data']

    col_a, col_b = st.sidebar.columns(2)

    with col_a:
        try:
            excel_data = df_to_excel_bytes(df)
            st.download_button(
                label="ğŸ“¥ Excel",
                data=excel_data,
                file_name=f"ì‚°ë¦¼ì²­_ì…ì°°_{selected_history['timestamp']}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                key=f"sidebar_excel_{selected_history['timestamp']}"
            )
        except Exception as e:
            st.error(f"Excel ìƒì„± ì‹¤íŒ¨: {e}")

    with col_b:
        try:
            csv_data = df_to_csv_bytes(df)
            st.download_button(
                label="ğŸ“¥ CSV",
                data=csv_data,
                file_name=f"ì‚°ë¦¼ì²­_ì…ì°°_{selected_history['timestamp']}.csv",
                mime="text/csv",
                key=f"sidebar_csv_{selected_history['timestamp']}"
            )
        except Exception as e:
            st.error(f"CSV ìƒì„± ì‹¤íŒ¨: {e}")

# ì‚¬ì´ë“œë°” í•˜ë‹¨: í¬ë¡¤ë§ íˆìŠ¤í† ë¦¬
if st.session_state.crawl_history:
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“‚ ì´ì „ í¬ë¡¤ë§ ê²°ê³¼")

    for idx, item in enumerate(reversed(st.session_state.crawl_history)):
        # timestampë¥¼ unique keyë¡œ ì‚¬ìš©
        unique_key = item['timestamp'].replace(':', '').replace(' ', '').replace('-', '')

        with st.sidebar.expander(f"ğŸ• {item['timestamp'].replace('_', ' ')}", expanded=False):
            st.write(f"**ìˆ˜ì§‘ ê¸°ê°„**: {item['period']}")
            st.write(f"**í•­ëª© ìˆ˜**: {item['total_items']}ê°œ")

            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            col_a, col_b = st.columns(2)

            with col_a:
                # Excel ë‹¤ìš´ë¡œë“œ (ìºì‹± ì‚¬ìš©)
                try:
                    excel_data = df_to_excel_bytes(item['data'])
                    st.download_button(
                        label="ğŸ“¥ Excel",
                        data=excel_data,
                        file_name=f"ì‚°ë¦¼ì²­_ì…ì°°_{item['timestamp']}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key=f"excel_{unique_key}"
                    )
                except Exception as e:
                    st.error(f"Excel ìƒì„± ì‹¤íŒ¨: {e}")

            with col_b:
                # CSV ë‹¤ìš´ë¡œë“œ (ìºì‹± ì‚¬ìš©)
                try:
                    csv = df_to_csv_bytes(item['data'])
                    st.download_button(
                        label="ğŸ“¥ CSV",
                        data=csv,
                        file_name=f"ì‚°ë¦¼ì²­_ì…ì°°_{item['timestamp']}.csv",
                        mime="text/csv",
                        key=f"csv_{unique_key}"
                    )
                except Exception as e:
                    st.error(f"CSV ìƒì„± ì‹¤íŒ¨: {e}")

# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([2, 1])

with col1:
    st.info(f"""
    ğŸ“… **ìˆ˜ì§‘ ê¸°ê°„**: {start_date} ~ {end_date} (ì´ {days}ì¼)
    â±ï¸ **ìš”ì²­ ë”œë ˆì´**: {delay}ì´ˆ
    ğŸ“„ **í˜ì´ì§€ ë”œë ˆì´**: {page_delay}ì´ˆ
    ğŸ¯ **ëŒ€ìƒ**: ì‚°ë¦¼ì²­ ì…ì°°ê³µê³  ê²Œì‹œíŒ
    """)

with col2:
    st.metric("ì‹œì‘ì¼", start_date.strftime('%Y-%m-%d'))
    st.metric("ì¢…ë£Œì¼", end_date.strftime('%Y-%m-%d'))

# í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜ (ì¤‘ë³µ ì œê±°)
def run_crawling(start_date, end_date, days, delay, page_delay):
    """í¬ë¡¤ë§ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
    # ê¸°ê°„ ì •ë³´ ì €ì¥
    period_str = f"{start_date} ~ {end_date}"

    # ì§„í–‰ ìƒí™© í‘œì‹œ ì˜ì—­
    progress_bar = st.progress(0)
    status_text = st.empty()
    info_text = st.empty()
    result_placeholder = st.empty()

    try:
        add_log(f"í¬ë¡¤ë§ ì‹œì‘ - ìˆ˜ì§‘ ê¸°ê°„: {period_str} ({days}ì¼)")
        add_log(f"ì„¤ì • - ìš”ì²­ ë”œë ˆì´: {delay}ì´ˆ, í˜ì´ì§€ ë”œë ˆì´: {page_delay}ì´ˆ")

        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = ForestBidCrawler(
            days=days,
            delay=delay,
            page_delay=page_delay
        )

        status_text.info("ğŸ”„ í¬ë¡¤ë§ ì‹œì‘...")

        # í¬ë¡¤ë§ ì‹¤í–‰
        page_index = 1
        should_continue = True
        total_pages_estimate = 50

        while should_continue:
            status_text.info(f"ğŸ“„ í˜ì´ì§€ {page_index} ì²˜ë¦¬ ì¤‘...")
            info_text.info(f"ğŸ” í˜ì´ì§€ {page_index} í•­ëª© ë¶„ì„ ì¤‘...")
            add_log(f"í˜ì´ì§€ {page_index} ì²˜ë¦¬ ì‹œì‘")

            # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            params = {
                'mn': 'NKFS_04_01_04',
                'bbsId': 'BBSMSTR_1033',
                'pageIndex': page_index,
                'pageUnit': 10
            }

            soup = crawler.fetch_page(crawler.LIST_URL, params)

            if not soup:
                add_log(f"í˜ì´ì§€ {page_index} ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨", "ERROR")
                break

            items = crawler.parse_list_page(soup)

            if not items:
                add_log(f"í˜ì´ì§€ {page_index}ì— í•­ëª© ì—†ìŒ", "WARNING")
                break

            add_log(f"í˜ì´ì§€ {page_index}ì—ì„œ {len(items)}ê°œ í•­ëª© ë°œê²¬")

            # ê° í•­ëª© ì²˜ë¦¬
            for idx, item in enumerate(items, 1):
                # ìƒë‹¨ ê³ ì • ê³µì§€ëŠ” ë²ˆí˜¸ê°€ ë¹„ê±°ë‚˜ 'ê³µì§€' í‘œê¸°ë¡œ ë‚˜íƒ€ë‚˜ë¯€ë¡œ ê±´ë„ˆë›´ë‹¤.
                number_text = str(item.get('number', '')).strip()
                is_notice = not number_text or 'ê³µì§€' in number_text

                # ë‚ ì§œ ì²´í¬ (ê³µì§€ ì œì™¸)
                if item['post_date'] and item['post_date'] < crawler.cutoff_date and not is_notice:
                    add_log(f"ê¸°ì¤€ì¼ ì´ì „ ê²Œì‹œê¸€ ë„ë‹¬ ({item['post_date_str']}) - í¬ë¡¤ë§ ì¢…ë£Œ", "INFO")
                    should_continue = False
                    break

                # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                if item['detail_url']:
                    time.sleep(crawler.delay)
                    detail_soup = crawler.fetch_page(item['detail_url'])

                    if detail_soup:
                        detail_data = crawler.parse_detail_page(detail_soup, item)
                        crawler.data.append(detail_data)
                        crawler.total_items += 1
                        add_log(f"í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ: {item['title'][:30]}...")
                        info_text.text(f"âœ… {page_index}í˜ì´ì§€ {idx}/10 ì²˜ë¦¬ ì™„ë£Œ: {item['title'][:30]}...")
                    else:
                        add_log(f"ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {item['title'][:30]}...", "ERROR")
                        crawler.data.append(item)
                        crawler.total_items += 1
                        info_text.text(f"âš ï¸ ìƒì„¸ í˜ì´ì§€ ì‹¤íŒ¨: {item['title'][:30]}...")
                else:
                    crawler.data.append(item)
                    crawler.total_items += 1
                    info_text.text(f"â„¹ï¸ ìƒì„¸ í˜ì´ì§€ ë§í¬ ì—†ìŒ: {item['title'][:30]}...")

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = min(page_index / total_pages_estimate, 0.99)
                progress_bar.progress(progress)

            # ì¤‘ê°„ ê²°ê³¼ í‘œì‹œ
            if crawler.data:
                df = pd.DataFrame(crawler.data)
                latest_rows = df.tail(20)

                preview_columns = [
                    ('number', 'ë²ˆí˜¸'),
                    ('title', 'ì œëª©'),
                    ('forest_office', 'ë‹´ë‹¹ì‚°ë¦¼ì²­'),
                    ('department', 'ë‹´ë‹¹ë¶€ì„œ'),
                    ('manager', 'ë‹´ë‹¹ì'),
                    ('contact', 'ì—°ë½ì²˜'),
                    ('post_date_str', 'ê³µê³ ì¼ì'),
                    ('views', 'ì¡°íšŒìˆ˜'),
                    ('has_attachment', 'ì²¨ë¶€'),
                ]

                available_preview_cols = [col for col, _ in preview_columns if col in latest_rows.columns]
                preview_df = latest_rows[available_preview_cols].copy()
                rename_map = {col: label for col, label in preview_columns if col in preview_df.columns}
                preview_df = preview_df.rename(columns=rename_map)

                result_placeholder.dataframe(
                    preview_df,
                    use_container_width=True,
                    hide_index=True
                )

            if should_continue:
                page_index += 1
                time.sleep(crawler.page_delay)

            # ìµœëŒ€ í˜ì´ì§€ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€) - 500í˜ì´ì§€ = 5000ê°œ í•­ëª©
            if page_index > 500:
                add_log("ìµœëŒ€ í˜ì´ì§€ ìˆ˜(500) ë„ë‹¬ - í¬ë¡¤ë§ ì¢…ë£Œ", "WARNING")
                break

        progress_bar.progress(1.0)
        status_text.success(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {crawler.total_items}ê°œ í•­ëª© ìˆ˜ì§‘")
        add_log(f"í¬ë¡¤ë§ ì™„ë£Œ - ì´ {crawler.total_items}ê°œ í•­ëª© ìˆ˜ì§‘")

        # ë°ì´í„° ì²˜ë¦¬ ë° ë°˜í™˜
        if crawler.data:
            df = pd.DataFrame(crawler.data)

            # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
            columns = [
                'number', 'title', 'forest_office', 'department',
                'manager', 'contact', 'post_date_str', 'views',
                'has_attachment', 'detail_url'
            ]
            columns = [col for col in columns if col in df.columns]
            df = df[columns]

            # ì»¬ëŸ¼ëª… í•œê¸€í™”
            df.columns = [
                'ë²ˆí˜¸', 'ì œëª©', 'ë‹´ë‹¹ì‚°ë¦¼ì²­', 'ë‹´ë‹¹ë¶€ì„œ',
                'ë‹´ë‹¹ì', 'ì—°ë½ì²˜', 'ê³µê³ ì¼ì', 'ì¡°íšŒìˆ˜',
                'ì²¨ë¶€', 'URL'
            ][:len(columns)]

            # ì„¸ì…˜ì— ë°ì´í„° ì €ì¥
            st.session_state.crawl_data = df
            st.session_state.crawl_completed = True

            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ìœ ì§€)
            history_item = {
                'timestamp': datetime.now().strftime('%Y-%m-%d_%H-%M-%S'),  # íŒŒì¼ëª… ì•ˆì „
                'data': df.copy(),
                'total_items': len(df),
                'period': period_str
            }
            st.session_state.crawl_history.append(history_item)

            # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if len(st.session_state.crawl_history) > 5:
                st.session_state.crawl_history.pop(0)  # ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°

            # ê²°ê³¼ í‘œì‹œ
            st.markdown("---")
            st.subheader("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")

            # í†µê³„ ì •ë³´
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ í•­ëª© ìˆ˜", len(df))
            with col2:
                st.metric("ë‹´ë‹¹ì‚°ë¦¼ì²­ ìˆ˜", df['ë‹´ë‹¹ì‚°ë¦¼ì²­'].nunique() if 'ë‹´ë‹¹ì‚°ë¦¼ì²­' in df.columns else 0)
            with col3:
                st.metric("ìˆ˜ì§‘ í˜ì´ì§€", page_index)
            with col4:
                # í‰ê·  ì¡°íšŒìˆ˜ ê³„ì‚° (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
                avg_views = 0
                if 'ì¡°íšŒìˆ˜' in df.columns and len(df) > 0:
                    try:
                        views_numbers = df['ì¡°íšŒìˆ˜'].astype(str).str.extract('(\d+)')[0].astype(float)
                        avg_views = int(views_numbers.mean()) if not views_numbers.isna().all() else 0
                    except:
                        avg_views = 0
                st.metric("í‰ê·  ì¡°íšŒìˆ˜", avg_views)

            # ë°ì´í„° í…Œì´ë¸”
            st.dataframe(df, use_container_width=True, hide_index=True)

            return True
        else:
            return False

    except Exception as e:
        status_text.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        add_log(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
        st.exception(e)
        return False

# í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼
col_btn1, col_btn2 = st.columns(2)

with col_btn1:
    start_crawl = st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", type="primary", use_container_width=True)

with col_btn2:
    export_data = st.button("ğŸ“¥ í¬ë¡¤ë§ ë° ì™„ë£Œì‹œ ì—‘ì…€íŒŒì¼ ì‘ì„±", type="secondary", use_container_width=True)

# í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼
if start_crawl:
    # ì´ˆê¸°í™”
    st.session_state.crawl_logs = []
    st.session_state.crawl_data = None
    st.session_state.crawl_completed = False

    # í¬ë¡¤ë§ ì‹¤í–‰
    run_crawling(start_date, end_date, days, delay, page_delay)

# "í¬ë¡¤ë§ ë° ì™„ë£Œì‹œ ì—‘ì…€íŒŒì¼ ì‘ì„±" ë²„íŠ¼ ê¸°ëŠ¥
if export_data:
    # ì´ˆê¸°í™”
    st.session_state.crawl_logs = []
    st.session_state.crawl_data = None
    st.session_state.crawl_completed = False

    # í¬ë¡¤ë§ ì‹¤í–‰
    run_crawling(start_date, end_date, days, delay, page_delay)

# í¬ë¡¤ë§ ì™„ë£Œ í›„ ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ (ë‘ ë²„íŠ¼ ëª¨ë‘ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)
if st.session_state.crawl_completed and st.session_state.crawl_data is not None:
    st.markdown("---")
    st.subheader("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")

    df = st.session_state.crawl_data
    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
    col1, col2 = st.columns(2)

    with col1:
        # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
        try:
            excel_data = df_to_excel_bytes(df)

            st.download_button(
                label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.xlsx)",
                data=excel_data,
                file_name=f"ì‚°ë¦¼ì²­_ì…ì°°ì •ë³´_{timestamp.replace('-', '').replace('_', '')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
        except Exception as e:
            st.error(f"ì—‘ì…€ ìƒì„± ì‹¤íŒ¨: {e}")

    with col2:
        # CSV ë‹¤ìš´ë¡œë“œ
        try:
            csv_data = df_to_csv_bytes(df)

            st.download_button(
                label="ğŸ“¥ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ (.csv)",
                data=csv_data,
                file_name=f"ì‚°ë¦¼ì²­_ì…ì°°ì •ë³´_{timestamp.replace('-', '').replace('_', '')}.csv",
                mime="text/csv",
                use_container_width=True
            )
        except Exception as e:
            st.error(f"CSV ìƒì„± ì‹¤íŒ¨: {e}")

# ë¡œê·¸ ë·°ì–´ (ì¢Œì¸¡ í•˜ë‹¨)
if st.session_state.crawl_logs and len(st.session_state.crawl_logs) > 0:
    st.markdown("---")

    log_col1, log_col2 = st.columns([3, 1])

    with log_col1:
        st.subheader("ğŸ“‹ í¬ë¡¤ë§ ë¡œê·¸")

    with log_col2:
        # ë¡œê·¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
        log_content = "# ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ë§ ë¡œê·¸\n\n"
        log_content += f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        log_content += "## ë¡œê·¸ ë‚´ì—­\n\n"
        log_content += "\n".join(st.session_state.crawl_logs)

        st.download_button(
            label="ğŸ“¥ ë¡œê·¸ ë‹¤ìš´ë¡œë“œ (.md)",
            data=log_content,
            file_name=f"í¬ë¡¤ë§_ë¡œê·¸_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
            mime="text/markdown",
            use_container_width=True
        )

    # ë¡œê·¸ í‘œì‹œ (í™•ì¥ ê°€ëŠ¥í•œ í˜•íƒœ)
    with st.expander("ë¡œê·¸ ë³´ê¸°", expanded=False):
        log_text = "\n".join(st.session_state.crawl_logs)
        st.text_area(
            "ë¡œê·¸ ë‚´ìš©",
            value=log_text,
            height=300,
            disabled=True,
            label_visibility="collapsed"
        )

# ì‚¬ìš© ì•ˆë‚´
st.markdown("---")
st.markdown("""
### ğŸ“– ì‚¬ìš© ì•ˆë‚´

1. **ì™¼ìª½ ì‚¬ì´ë“œë°”**ì—ì„œ í¬ë¡¤ë§ ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”.
2. **í¬ë¡¤ë§ ì‹œì‘** ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.
3. ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. ì™„ë£Œ í›„ **ì—‘ì…€/CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ** ë²„íŠ¼ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì„¸ìš”.
5. **ë¡œê·¸ ë³´ê¸°**ì—ì„œ í¬ë¡¤ë§ ê³¼ì •ì„ í™•ì¸í•˜ê³ , ë¡œê·¸ë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### âš ï¸ ì£¼ì˜ì‚¬í•­

- í¬ë¡¤ë§ì— ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ìˆ˜ì§‘ ê¸°ê°„ì— ë”°ë¼ ìˆ˜ ë¶„ ~ ìˆ˜ì‹­ ë¶„)
- ì„œë²„ ë¶€í•˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì ì ˆí•œ ë”œë ˆì´ë¥¼ ì„¤ì •í•˜ì„¸ìš”
- ê³µê°œëœ ì •ë³´ë§Œ ìˆ˜ì§‘í•˜ë©°, ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”

### ğŸ“Š ìˆ˜ì§‘ ì •ë³´

- ë²ˆí˜¸, ì œëª©, ë‹´ë‹¹ì‚°ë¦¼ì²­, ë‹´ë‹¹ë¶€ì„œ
- ë‹´ë‹¹ì, ì—°ë½ì²˜, ê³µê³ ì¼ì, ì¡°íšŒìˆ˜
- ì²¨ë¶€íŒŒì¼ ì—¬ë¶€, ìƒì„¸ í˜ì´ì§€ URL
""")

# í‘¸í„°
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: gray;'>"
    "ğŸŒ² ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬ v1.0 | "
    "ì‚°ë¦¼ê³µí•™ ì „ë¬¸ê°€ / ì‚°ë¦¼í•™ ì—°êµ¬ì"
    "</div>",
    unsafe_allow_html=True
)
