#!/usr/bin/env ts-node
/**
 * ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
 *
 * ì‚¬ìš©ë²•:
 *   npx ts-node src/scripts/runCrawler.ts [options]
 *
 * ì˜µì…˜:
 *   --crawler=<type>   íŠ¹ì • í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰ (nec_policy, party_minjoo, party_ppp)
 *   --download         PDF ë‹¤ìš´ë¡œë“œ í™œì„±í™”
 *   --pages=<n>        í¬ë¡¤ë§í•  í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 3)
 *   --dry-run          ì‹¤ì œ ì €ì¥ ì—†ì´ í…ŒìŠ¤íŠ¸
 */

import { CrawlerFactory } from '../services/crawler/crawlerFactory';
import { CrawlOptions, CrawlResult, CrawlerType } from '../types/crawler.types';
import fs from 'fs/promises';
import path from 'path';

// ì„¤ì •
const BASE_OUTPUT_DIR = path.resolve(__dirname, '../../../data/crawled');
const LOG_DIR = path.resolve(__dirname, '../../../data/crawled/logs');

// í¬ë¡¤ë§ ëŒ€ìƒ ëª©ë¡
const CRAWL_TARGETS: CrawlerType[] = [
  'nec_policy',
  'party_minjoo',
  'party_ppp',
];

interface CrawlStats {
  crawler: string;
  status: 'success' | 'partial' | 'failed';
  itemCount: number;
  downloadedFiles: number;
  errors: number;
  durationMs: number;
}

async function ensureDirectories(): Promise<void> {
  await fs.mkdir(LOG_DIR, { recursive: true });
  for (const target of CRAWL_TARGETS) {
    await fs.mkdir(path.join(BASE_OUTPUT_DIR, target, 'pdf'), { recursive: true });
    await fs.mkdir(path.join(BASE_OUTPUT_DIR, target, 'json'), { recursive: true });
  }
}

async function saveResults(
  crawlerType: string,
  result: CrawlResult
): Promise<void> {
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
  const jsonPath = path.join(
    BASE_OUTPUT_DIR,
    crawlerType,
    'json',
    `crawl_${timestamp}.json`
  );

  await fs.writeFile(jsonPath, JSON.stringify(result, null, 2), 'utf-8');
  console.log(`  ğŸ“ ê²°ê³¼ ì €ì¥: ${jsonPath}`);
}

async function runCrawler(
  crawlerType: CrawlerType,
  options: CrawlOptions
): Promise<CrawlStats> {
  console.log(`\nğŸš€ [${crawlerType}] í¬ë¡¤ë§ ì‹œì‘...`);
  const startTime = Date.now();

  try {
    const crawler = CrawlerFactory.create(crawlerType);
    const result = await crawler.crawl(options);

    // ê²°ê³¼ ì €ì¥
    await saveResults(crawlerType, result);

    const stats: CrawlStats = {
      crawler: crawlerType,
      status: result.success ? 'success' : 'partial',
      itemCount: result.itemCount,
      downloadedFiles: result.downloadedFiles?.length || 0,
      errors: result.errors?.length || 0,
      durationMs: Date.now() - startTime,
    };

    console.log(`  âœ… ì™„ë£Œ: ${stats.itemCount}ê°œ í•­ëª©, ${stats.downloadedFiles}ê°œ íŒŒì¼`);
    if (stats.errors > 0) {
      console.log(`  âš ï¸  ì—ëŸ¬: ${stats.errors}ê°œ`);
    }

    return stats;
  } catch (error: any) {
    console.error(`  âŒ ì‹¤íŒ¨: ${error.message}`);
    return {
      crawler: crawlerType,
      status: 'failed',
      itemCount: 0,
      downloadedFiles: 0,
      errors: 1,
      durationMs: Date.now() - startTime,
    };
  }
}

async function main(): Promise<void> {
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('   naPO ëŒ€ê·œëª¨ í¬ë¡¤ë§ ì‹œì‘');
  console.log('   ì‹œì‘ ì‹œê°„:', new Date().toLocaleString('ko-KR'));
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

  // ì¸ì íŒŒì‹±
  const args = process.argv.slice(2);
  const targetCrawler = args.find(a => a.startsWith('--crawler='))?.split('=')[1];
  const downloadFiles = args.includes('--download');
  const pagesArg = args.find(a => a.startsWith('--pages='))?.split('=')[1];
  const endPage = pagesArg ? parseInt(pagesArg, 10) : 3;
  const dryRun = args.includes('--dry-run');

  if (dryRun) {
    console.log('\nâš ï¸  DRY RUN ëª¨ë“œ - ì‹¤ì œ ì €ì¥ ì—†ìŒ\n');
  }

  // ë””ë ‰í† ë¦¬ ìƒì„±
  await ensureDirectories();

  // í¬ë¡¤ë§ ì˜µì…˜
  const crawlOptions: CrawlOptions = {
    startPage: 1,
    endPage,
    downloadFiles,
    outputDir: BASE_OUTPUT_DIR,
    filters: {
      includePolicy: true,
      includeCandidates: false, // ì„ ê±° ê¸°ê°„ ì•„ë‹ˆë©´ false
    },
  };

  console.log('\nğŸ“‹ í¬ë¡¤ë§ ì„¤ì •:');
  console.log(`   - í˜ì´ì§€ ë²”ìœ„: 1~${endPage}`);
  console.log(`   - PDF ë‹¤ìš´ë¡œë“œ: ${downloadFiles ? 'í™œì„±í™”' : 'ë¹„í™œì„±í™”'}`);
  console.log(`   - ëŒ€ìƒ: ${targetCrawler || 'ëª¨ë“  í¬ë¡¤ëŸ¬'}`);

  // í¬ë¡¤ë§ ì‹¤í–‰
  const targets = targetCrawler
    ? [targetCrawler as CrawlerType]
    : CRAWL_TARGETS;

  const allStats: CrawlStats[] = [];

  for (const target of targets) {
    const outputDir = path.join(BASE_OUTPUT_DIR, target, 'pdf');
    const stats = await runCrawler(target, {
      ...crawlOptions,
      outputDir,
    });
    allStats.push(stats);

    // í¬ë¡¤ëŸ¬ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
    if (targets.indexOf(target) < targets.length - 1) {
      console.log('\nâ³ ë‹¤ìŒ í¬ë¡¤ëŸ¬ ëŒ€ê¸° (5ì´ˆ)...');
      await new Promise(resolve => setTimeout(resolve, 5000));
    }
  }

  // ìµœì¢… ë³´ê³ ì„œ
  console.log('\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('   í¬ë¡¤ë§ ì™„ë£Œ ë³´ê³ ì„œ');
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

  let totalItems = 0;
  let totalFiles = 0;
  let totalErrors = 0;
  let totalDuration = 0;

  for (const stats of allStats) {
    const statusIcon =
      stats.status === 'success' ? 'âœ…' :
      stats.status === 'partial' ? 'âš ï¸' : 'âŒ';

    console.log(`\n${statusIcon} ${stats.crawler}:`);
    console.log(`   - í•­ëª©: ${stats.itemCount}ê°œ`);
    console.log(`   - íŒŒì¼: ${stats.downloadedFiles}ê°œ`);
    console.log(`   - ì—ëŸ¬: ${stats.errors}ê°œ`);
    console.log(`   - ì†Œìš”: ${(stats.durationMs / 1000).toFixed(1)}ì´ˆ`);

    totalItems += stats.itemCount;
    totalFiles += stats.downloadedFiles;
    totalErrors += stats.errors;
    totalDuration += stats.durationMs;
  }

  console.log('\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€');
  console.log(`ğŸ“Š ì´ê³„:`);
  console.log(`   - ì´ í•­ëª©: ${totalItems}ê°œ`);
  console.log(`   - ì´ íŒŒì¼: ${totalFiles}ê°œ`);
  console.log(`   - ì´ ì—ëŸ¬: ${totalErrors}ê°œ`);
  console.log(`   - ì´ ì†Œìš”: ${(totalDuration / 1000 / 60).toFixed(1)}ë¶„`);
  console.log(`   - ì¢…ë£Œ ì‹œê°„: ${new Date().toLocaleString('ko-KR')}`);
  console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');

  // ë¡œê·¸ ì €ì¥
  const logPath = path.join(
    LOG_DIR,
    `crawl_${new Date().toISOString().split('T')[0]}.log`
  );
  const logContent = {
    startTime: new Date(Date.now() - totalDuration).toISOString(),
    endTime: new Date().toISOString(),
    stats: allStats,
    totals: { totalItems, totalFiles, totalErrors, totalDuration },
  };
  await fs.writeFile(logPath, JSON.stringify(logContent, null, 2), 'utf-8');
  console.log(`ğŸ“ ë¡œê·¸ ì €ì¥: ${logPath}`);
}

main().catch(console.error);
