#!/usr/bin/env python3
"""
226ê°œ ê¸°ì´ˆì§€ìì²´ ì „ìˆ˜ì¡°ì‚¬ í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
ê° ì˜íšŒë‹¹ ìµœê·¼ 50ê°œ íšŒì˜ë¡ ìˆ˜ì§‘
"""

import sys
import os
import json
import time
import traceback
from datetime import datetime
from pathlib import Path

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from council_crawler import load_basic_councils_from_yaml, get_crawler, ResultSaver

def crawl_all_basic_councils(max_pages=5, output_dir="output/basic_minutes"):
    """226ê°œ ê¸°ì´ˆì§€ìì²´ ì „ìˆ˜ì¡°ì‚¬"""

    # ê¸°ì´ˆì˜íšŒ ëª©ë¡ ë¡œë“œ
    basic_councils = load_basic_councils_from_yaml()

    print(f"=" * 80)
    print(f"ğŸ“Š 226ê°œ ê¸°ì´ˆì§€ìì²´ ì „ìˆ˜ì¡°ì‚¬ í¬ë¡¤ë§")
    print(f"=" * 80)
    print(f"ëŒ€ìƒ: {len(basic_councils)}ê°œ ê¸°ì´ˆì§€ìì²´")
    print(f"í˜ì´ì§€ë‹¹ íšŒì˜ë¡: ìµœëŒ€ {max_pages * 10}ê°œ (max_pages={max_pages})")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"=" * 80)

    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    saver = ResultSaver(output_dir)

    # ê²°ê³¼ ì§‘ê³„
    results_summary = []
    success_count = 0
    fail_count = 0
    total_meetings = 0
    total_size = 0
    truncation_issues = []  # 500ì ì´í•˜ ë¬¸ì œ

    # ì§„í–‰ ìƒíƒœ íŒŒì¼
    progress_file = Path(output_dir) / "_progress.json"
    completed_codes = set()

    # ì´ì „ ì§„í–‰ ìƒíƒœ ë¡œë“œ
    if progress_file.exists():
        with open(progress_file, 'r') as f:
            progress_data = json.load(f)
            completed_codes = set(progress_data.get('completed', []))
            print(f"â© ì´ì „ ì§„í–‰ ìƒíƒœ ë¡œë“œ: {len(completed_codes)}ê°œ ì™„ë£Œë¨, ì´ì–´ì„œ ì§„í–‰")

    # ì •ë ¬ëœ ì½”ë“œ ëª©ë¡
    council_codes = sorted(basic_councils.keys())

    for idx, code in enumerate(council_codes, 1):
        config = basic_councils[code]
        name = config.get('name', code)
        crawler_type = config.get('crawler_type', 'default')
        note = config.get('note', '')

        # ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ìŠ¤í‚µ
        if code in completed_codes:
            print(f"[{idx:3d}/226] {name:<20} â­ï¸  ì´ë¯¸ ì™„ë£Œë¨")
            continue

        # SSL ë¬¸ì œ ë“± ì•Œë ¤ì§„ ì´ìŠˆ
        if 'SSL' in note or 'ì ‘ê·¼ ë¶ˆê°€' in note:
            print(f"[{idx:3d}/226] {name:<20} âš ï¸  {note}")
            results_summary.append({
                'code': code,
                'name': name,
                'crawler_type': crawler_type,
                'status': 'skip',
                'reason': note,
                'meetings': 0,
                'size_bytes': 0,
                'avg_content_len': 0,
                'min_content_len': 0,
                'truncation_count': 0
            })
            fail_count += 1
            continue

        print(f"[{idx:3d}/226] {name:<20} ({crawler_type:<12}) ", end="", flush=True)

        try:
            crawler = get_crawler(code)
            if not crawler:
                print("âŒ í¬ë¡¤ëŸ¬ ìƒì„± ì‹¤íŒ¨")
                results_summary.append({
                    'code': code,
                    'name': name,
                    'crawler_type': crawler_type,
                    'status': 'fail',
                    'reason': 'í¬ë¡¤ëŸ¬ ìƒì„± ì‹¤íŒ¨',
                    'meetings': 0,
                    'size_bytes': 0,
                    'avg_content_len': 0,
                    'min_content_len': 0,
                    'truncation_count': 0
                })
                fail_count += 1
                continue

            # í¬ë¡¤ë§ ì‹¤í–‰
            meetings = []
            content_lengths = []
            truncation_count = 0

            for meeting in crawler.crawl(max_pages=max_pages):
                meetings.append(meeting)
                fc_len = len(meeting.full_content) if meeting.full_content else 0
                content_lengths.append(fc_len)

                # 500ì ì´í•˜ ì²´í¬ (truncation ë¬¸ì œ ê°ì§€)
                if fc_len > 0 and fc_len < 500:
                    truncation_count += 1

            if meetings:
                # ì €ì¥
                md_file = saver.save_markdown(code, meetings)
                jsonl_file = saver.save_jsonl(code, meetings)

                file_size = md_file.stat().st_size if md_file.exists() else 0
                avg_len = sum(content_lengths) / len(content_lengths) if content_lengths else 0
                min_len = min(content_lengths) if content_lengths else 0

                print(f"âœ… {len(meetings):3d}ê°œ | {file_size/1024:7.1f}KB | avg:{avg_len:,.0f}ì", end="")

                if truncation_count > 0:
                    print(f" | âš ï¸ {truncation_count}ê°œ <500ì")
                    truncation_issues.append({
                        'code': code,
                        'name': name,
                        'crawler_type': crawler_type,
                        'truncation_count': truncation_count,
                        'total': len(meetings),
                        'min_len': min_len
                    })
                else:
                    print()

                results_summary.append({
                    'code': code,
                    'name': name,
                    'crawler_type': crawler_type,
                    'status': 'success',
                    'reason': '',
                    'meetings': len(meetings),
                    'size_bytes': file_size,
                    'avg_content_len': avg_len,
                    'min_content_len': min_len,
                    'truncation_count': truncation_count
                })

                success_count += 1
                total_meetings += len(meetings)
                total_size += file_size

                # ì§„í–‰ ìƒíƒœ ì €ì¥
                completed_codes.add(code)
                with open(progress_file, 'w') as f:
                    json.dump({
                        'completed': list(completed_codes),
                        'last_update': datetime.now().isoformat()
                    }, f)
            else:
                print(f"âš ï¸  íšŒì˜ë¡ 0ê°œ")
                results_summary.append({
                    'code': code,
                    'name': name,
                    'crawler_type': crawler_type,
                    'status': 'empty',
                    'reason': 'íšŒì˜ë¡ ì—†ìŒ',
                    'meetings': 0,
                    'size_bytes': 0,
                    'avg_content_len': 0,
                    'min_content_len': 0,
                    'truncation_count': 0
                })
                fail_count += 1

            # Rate limiting
            time.sleep(0.5)

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {str(e)[:50]}")
            results_summary.append({
                'code': code,
                'name': name,
                'crawler_type': crawler_type,
                'status': 'error',
                'reason': str(e)[:100],
                'meetings': 0,
                'size_bytes': 0,
                'avg_content_len': 0,
                'min_content_len': 0,
                'truncation_count': 0
            })
            fail_count += 1

            # ì˜¤ë¥˜ ë¡œê·¸
            with open(Path(output_dir) / "_errors.log", 'a') as f:
                f.write(f"\n{'='*60}\n")
                f.write(f"[{datetime.now()}] {code} - {name}\n")
                f.write(traceback.format_exc())

    # ìµœì¢… ê²°ê³¼ ì €ì¥
    summary_file = Path(output_dir) / "_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now().isoformat(),
            'total_councils': len(basic_councils),
            'success_count': success_count,
            'fail_count': fail_count,
            'total_meetings': total_meetings,
            'total_size_bytes': total_size,
            'total_size_mb': total_size / 1024 / 1024,
            'truncation_issues': truncation_issues,
            'results': results_summary
        }, f, ensure_ascii=False, indent=2)

    # ìµœì¢… ë³´ê³ 
    print(f"\n{'=' * 80}")
    print(f"ğŸ“Š ì „ìˆ˜ì¡°ì‚¬ ì™„ë£Œ")
    print(f"{'=' * 80}")
    print(f"ì„±ê³µ: {success_count}ê°œ / ì‹¤íŒ¨: {fail_count}ê°œ")
    print(f"ì´ íšŒì˜ë¡: {total_meetings:,}ê°œ")
    print(f"ì´ ìš©ëŸ‰: {total_size/1024/1024:.1f}MB")
    print(f"í‰ê·  ìš©ëŸ‰/ì˜íšŒ: {total_size/success_count/1024:.1f}KB" if success_count > 0 else "")

    if truncation_issues:
        print(f"\nâš ï¸  500ì ë¯¸ë§Œ truncation ì˜ì‹¬: {len(truncation_issues)}ê°œ ì˜íšŒ")
        for issue in truncation_issues[:10]:
            print(f"   - {issue['name']}: {issue['truncation_count']}/{issue['total']}ê°œ (min: {issue['min_len']}ì)")

    print(f"\nê²°ê³¼ ì €ì¥: {summary_file}")
    print(f"ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    return results_summary

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description='226ê°œ ê¸°ì´ˆì§€ìì²´ ì „ìˆ˜ì¡°ì‚¬')
    parser.add_argument('--max-pages', type=int, default=5, help='í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸: 5, ì•½ 50ê°œ)')
    parser.add_argument('--output', type=str, default='output/basic_minutes', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    args = parser.parse_args()

    crawl_all_basic_councils(max_pages=args.max_pages, output_dir=args.output)
