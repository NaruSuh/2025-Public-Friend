#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬ - Streamlit Web App
"""

import streamlit as st
import pandas as pd
from datetime import datetime, timedelta
import time
from main import ForestBidCrawler
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬",
    page_icon="ğŸŒ²",
    layout="wide"
)

# ì œëª©
st.title("ğŸŒ² ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬")
st.markdown("---")

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.header("âš™ï¸ í¬ë¡¤ë§ ì„¤ì •")

# ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì •
days = st.sidebar.slider(
    "ìˆ˜ì§‘ ê¸°ê°„ (ì¼)",
    min_value=7,
    max_value=365,
    value=30,
    step=7,
    help="ìµœê·¼ ë©°ì¹  ë™ì•ˆì˜ ì…ì°°ì •ë³´ë¥¼ ìˆ˜ì§‘í• ì§€ ì„¤ì •í•©ë‹ˆë‹¤."
)

# ìš”ì²­ ê°„ê²© ì„¤ì •
delay = st.sidebar.slider(
    "ìš”ì²­ ê°„ ë”œë ˆì´ (ì´ˆ)",
    min_value=0.5,
    max_value=3.0,
    value=1.0,
    step=0.5,
    help="ê° ìš”ì²­ ì‚¬ì´ì˜ ëŒ€ê¸° ì‹œê°„ì…ë‹ˆë‹¤. ë„ˆë¬´ ì§§ìœ¼ë©´ ì„œë²„ì—ì„œ ì°¨ë‹¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
)

page_delay = st.sidebar.slider(
    "í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì´ˆ)",
    min_value=1.0,
    max_value=5.0,
    value=2.0,
    step=0.5,
    help="í˜ì´ì§€ ì´ë™ ì‹œ ëŒ€ê¸° ì‹œê°„ì…ë‹ˆë‹¤."
)

# ë©”ì¸ ì˜ì—­
col1, col2 = st.columns([2, 1])

with col1:
    st.info(f"""
    ğŸ“… **ìˆ˜ì§‘ ê¸°ê°„**: ìµœê·¼ {days}ì¼
    â±ï¸ **ìš”ì²­ ë”œë ˆì´**: {delay}ì´ˆ
    ğŸ“„ **í˜ì´ì§€ ë”œë ˆì´**: {page_delay}ì´ˆ
    ğŸ¯ **ëŒ€ìƒ**: ì‚°ë¦¼ì²­ ì…ì°°ê³µê³  ê²Œì‹œíŒ
    """)

with col2:
    st.metric("ìˆ˜ì§‘ ê¸°ì¤€ì¼", (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d'))

# í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼
if st.button("ğŸš€ í¬ë¡¤ë§ ì‹œì‘", type="primary", use_container_width=True):

    # ì§„í–‰ ìƒí™© í‘œì‹œ ì˜ì—­
    progress_bar = st.progress(0)
    status_text = st.empty()
    info_text = st.empty()

    # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ ì˜ì—­
    result_placeholder = st.empty()

    try:
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = ForestBidCrawler(
            days=days,
            delay=delay,
            page_delay=page_delay
        )

        status_text.info("ğŸ”„ í¬ë¡¤ë§ ì‹œì‘...")

        # í¬ë¡¤ë§ ì‹¤í–‰ (ìˆ˜ì •ëœ ë²„ì „)
        page_index = 1
        should_continue = True
        total_pages_estimate = 50  # ì˜ˆìƒ í˜ì´ì§€ ìˆ˜

        while should_continue:
            info_text.text(f"ğŸ“„ í˜ì´ì§€ {page_index} ì²˜ë¦¬ ì¤‘...")

            # ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            params = {
                'mn': 'NKFS_04_01_04',
                'bbsId': 'BBSMSTR_1033',
                'pageIndex': page_index,
                'pageUnit': 10
            }

            soup = crawler.fetch_page(crawler.LIST_URL, params)

            if not soup:
                break

            items = crawler.parse_list_page(soup)

            if not items:
                break

            # ê° í•­ëª© ì²˜ë¦¬
            for idx, item in enumerate(items, 1):
                # ë‚ ì§œ ì²´í¬
                if item['post_date'] and item['post_date'] < crawler.cutoff_date:
                    should_continue = False
                    break

                # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                if item['detail_url']:
                    time.sleep(crawler.delay)
                    detail_soup = crawler.fetch_page(item['detail_url'])

                    if detail_soup:
                        detail_data = crawler.parse_detail_page(detail_soup, item)
                        crawler.data.append(detail_data)
                        crawler.total_items += 1
                else:
                    crawler.data.append(item)
                    crawler.total_items += 1

                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                progress = min(page_index / total_pages_estimate, 0.99)
                progress_bar.progress(progress)

            # ì¤‘ê°„ ê²°ê³¼ í‘œì‹œ
            if crawler.data:
                df = pd.DataFrame(crawler.data)
                result_placeholder.dataframe(
                    df.head(20),
                    use_container_width=True,
                    hide_index=True
                )

            if should_continue:
                page_index += 1
                time.sleep(crawler.page_delay)

            # ìµœëŒ€ í˜ì´ì§€ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            if page_index > 100:
                break

        progress_bar.progress(1.0)
        status_text.success(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {crawler.total_items}ê°œ í•­ëª© ìˆ˜ì§‘")

        # ìµœì¢… ê²°ê³¼ í‘œì‹œ
        if crawler.data:
            df = pd.DataFrame(crawler.data)

            # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
            columns = [
                'number', 'title', 'forest_office', 'department',
                'manager', 'contact', 'post_date_str', 'views',
                'has_attachment', 'detail_url'
            ]
            columns = [col for col in columns if col in df.columns]
            df = df[columns]

            # ì»¬ëŸ¼ëª… í•œê¸€í™”
            df.columns = [
                'ë²ˆí˜¸', 'ì œëª©', 'ë‹´ë‹¹ì‚°ë¦¼ì²­', 'ë‹´ë‹¹ë¶€ì„œ',
                'ë‹´ë‹¹ì', 'ì—°ë½ì²˜', 'ê³µê³ ì¼ì', 'ì¡°íšŒìˆ˜',
                'ì²¨ë¶€', 'URL'
            ][:len(columns)]

            # ê²°ê³¼ í‘œì‹œ
            st.markdown("---")
            st.subheader("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼")

            # í†µê³„ ì •ë³´
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ì´ í•­ëª© ìˆ˜", len(df))
            with col2:
                st.metric("ë‹´ë‹¹ì‚°ë¦¼ì²­ ìˆ˜", df['ë‹´ë‹¹ì‚°ë¦¼ì²­'].nunique() if 'ë‹´ë‹¹ì‚°ë¦¼ì²­' in df.columns else 0)
            with col3:
                st.metric("ìˆ˜ì§‘ í˜ì´ì§€", page_index)
            with col4:
                st.metric("í‰ê·  ì¡°íšŒìˆ˜", int(df['ì¡°íšŒìˆ˜'].astype(str).str.extract('(\d+)')[0].astype(float).mean()) if 'ì¡°íšŒìˆ˜' in df.columns else 0)

            # ë°ì´í„° í…Œì´ë¸”
            st.dataframe(df, use_container_width=True, hide_index=True)

            # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
            filename = f"ì‚°ë¦¼ì²­_ì…ì°°ì •ë³´_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"

            # ì—‘ì…€ íŒŒì¼ ìƒì„±
            df.to_excel(filename, index=False, engine='openpyxl')

            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            with open(filename, 'rb') as f:
                st.download_button(
                    label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=f.read(),
                    file_name=filename,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )

            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(filename)

    except Exception as e:
        status_text.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.exception(e)

# ì‚¬ìš© ì•ˆë‚´
st.markdown("---")
st.markdown("""
### ğŸ“– ì‚¬ìš© ì•ˆë‚´

1. **ì™¼ìª½ ì‚¬ì´ë“œë°”**ì—ì„œ í¬ë¡¤ë§ ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš”.
2. **í¬ë¡¤ë§ ì‹œì‘** ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.
3. ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
4. ì™„ë£Œ í›„ **ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ** ë²„íŠ¼ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•˜ì„¸ìš”.

### âš ï¸ ì£¼ì˜ì‚¬í•­

- í¬ë¡¤ë§ì— ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ìˆ˜ì§‘ ê¸°ê°„ì— ë”°ë¼ ìˆ˜ ë¶„ ~ ìˆ˜ì‹­ ë¶„)
- ì„œë²„ ë¶€í•˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì ì ˆí•œ ë”œë ˆì´ë¥¼ ì„¤ì •í•˜ì„¸ìš”
- ê³µê°œëœ ì •ë³´ë§Œ ìˆ˜ì§‘í•˜ë©°, ì—°êµ¬ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”

### ğŸ“Š ìˆ˜ì§‘ ì •ë³´

- ë²ˆí˜¸, ì œëª©, ë‹´ë‹¹ì‚°ë¦¼ì²­, ë‹´ë‹¹ë¶€ì„œ
- ë‹´ë‹¹ì, ì—°ë½ì²˜, ê³µê³ ì¼ì, ì¡°íšŒìˆ˜
- ì²¨ë¶€íŒŒì¼ ì—¬ë¶€, ìƒì„¸ í˜ì´ì§€ URL
""")

# í‘¸í„°
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: gray;'>"
    "ğŸŒ² ì‚°ë¦¼ì²­ ì…ì°°ì •ë³´ í¬ë¡¤ëŸ¬ v1.0 | "
    "ì‚°ë¦¼ê³µí•™ ì „ë¬¸ê°€ / ì‚°ë¦¼í•™ ì—°êµ¬ì"
    "</div>",
    unsafe_allow_html=True
)
